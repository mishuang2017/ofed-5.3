From: Valentine Fatiev <valentinef@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en_tc.c

Change-Id: Ifb6088bdec9fa087219e55a57b1d667b0b05c7f2
---
 .../mellanox/mlx5/core/en/tc_tun_common.c     |  21 +-
 .../net/ethernet/mellanox/mlx5/core/en_tc.c   | 380 +++++++++++++++++-
 2 files changed, 389 insertions(+), 12 deletions(-)

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun_common.c b/drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun_common.c
index xxxxxxx..xxxxxxx 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun_common.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun_common.c
@@ -393,14 +393,17 @@ void mlx5e_tc_update_neigh_used_value(struct mlx5e_neigh_hash_entry *nhe)
 	struct neighbour *n;
 	u64 lastuse;
 
-	if (m_neigh->family == AF_INET)
+	if (m_neigh->family == AF_INET) {
 		tbl = &arp_tbl;
-#if IS_ENABLED(CONFIG_IPV6)
-	else if (m_neigh->family == AF_INET6)
+#if defined(HAVE_IPV6_STUBS_H) && IS_ENABLED(CONFIG_IPV6)
+	} else if (m_neigh->family == AF_INET6) {
+		if (!ipv6_stub || !ipv6_stub->nd_tbl)
+			return;
 		tbl = ipv6_stub->nd_tbl;
 #endif
-	else
+	} else {
 		return;
+	}
 
 	/* mlx5e_get_next_valid_encap() releases previous encap before returning
 	 * next one.
@@ -439,7 +442,9 @@ void mlx5e_tc_update_neigh_used_value(struct mlx5e_neigh_hash_entry *nhe)
 		}
 	}
 
+#ifndef MLX_DISABLE_TRACEPOINTS
 	trace_mlx5e_tc_update_neigh_used_value(nhe, neigh_used);
+#endif
 
 	if (neigh_used) {
 		nhe->reported_lastuse = jiffies;
@@ -659,7 +664,9 @@ static bool is_duplicated_encap_entry(struct mlx5e_priv *priv,
 	for (i = 0; i < out_index; i++) {
 		if (flow->encaps[i].e != e)
 			continue;
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack, "can't duplicate encap action");
+#endif
 		netdev_err(priv->netdev, "can't duplicate encap action\n");
 		return true;
 	}
@@ -710,7 +717,9 @@ int mlx5e_attach_encap(struct mlx5e_priv *priv,
 	key.ip_tun_key = &tun_info->key;
 	key.tc_tunnel = mlx5e_get_tc_tun(mirred_dev);
 	if (!key.tc_tunnel) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack, "Unsupported tunnel");
+#endif
 		return -EOPNOTSUPP;
 	}
 
@@ -836,8 +845,10 @@ int mlx5e_attach_decap(struct mlx5e_priv *priv,
 	esw_attr = attr->esw_attr;
 
 	if (sizeof(parse_attr->eth) > MLX5_CAP_ESW(priv->mdev, max_encap_header_size)) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "encap header larger than max supported");
+#endif
 		return -EOPNOTSUPP;
 	}
 
@@ -1626,7 +1637,9 @@ int mlx5e_tc_fib_event(struct notifier_block *nb, unsigned long event, void *ptr
 		if (!IS_ERR_OR_NULL(fib_work)) {
 			queue_work(priv->wq, &fib_work->work);
 		} else if (IS_ERR(fib_work)) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(info->extack, "Failed to init fib work");
+#endif
 			mlx5_core_warn(priv->mdev, "Failed to init fib work, %ld\n",
 				       PTR_ERR(fib_work));
 		}
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
index xxxxxxx..xxxxxxx 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
@@ -2011,8 +2011,10 @@ enc_opts_is_dont_care_or_full_match(struct mlx5e_priv *priv,
 
 			if (opt->opt_class != htons(U16_MAX) ||
 			    opt->type != U8_MAX) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 				NL_SET_ERR_MSG(extack,
 					       "Partial match of tunnel options in chain > 0 isn't supported");
+#endif
 				netdev_warn(priv->netdev,
 					    "Partial match of tunnel options in chain > 0 isn't supported");
 				return -EOPNOTSUPP;
@@ -2043,7 +2045,11 @@ static int mlx5e_get_flow_tunnel_id(struct mlx5e_priv *priv,
 				    struct net_device *filter_dev)
 {
 	struct flow_rule *rule = flow_cls_offload_flow_rule(f);
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#else
+	struct netlink_ext_ack *extack = NULL;
+#endif
 	struct mlx5e_tc_mod_hdr_acts *mod_hdr_acts;
 	struct flow_match_enc_opts enc_opts_match;
 	struct tunnel_match_enc_opts tun_enc_opts;
@@ -2216,7 +2222,9 @@ static int parse_tunnel_attr(struct mlx5e_priv *priv,
 {
 	struct mlx5e_tc_tunnel *tunnel = mlx5e_get_tc_tun(filter_dev);
 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	bool needs_mapping, sets_mapping;
 	int err;
 
@@ -2229,8 +2237,10 @@ static int parse_tunnel_attr(struct mlx5e_priv *priv,
 
 	if ((needs_mapping || sets_mapping) &&
 	    !mlx5_eswitch_reg_c1_loopback_enabled(esw)) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG(extack,
 			       "Chains on tunnel devices isn't supported without register loopback support");
+#endif
 		netdev_warn(priv->netdev,
 			    "Chains on tunnel devices isn't supported without register loopback support");
 		return -EOPNOTSUPP;
@@ -2240,8 +2250,10 @@ static int parse_tunnel_attr(struct mlx5e_priv *priv,
 		err = mlx5e_tc_tun_parse(filter_dev, priv, spec, f,
 					 match_level);
 		if (err) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "Failed to parse tunnel attributes");
+#endif
 			netdev_warn(priv->netdev,
 				    "Failed to parse tunnel attributes");
 			return err;
@@ -2250,7 +2262,9 @@ static int parse_tunnel_attr(struct mlx5e_priv *priv,
 		/* With mpls over udp we decapsulate using packet reformat
 		 * object
 		 */
+#ifdef HAVE_NET_BAREUDP_H
 		if (!netif_is_bareudp(filter_dev))
+#endif
 			flow->attr->action |= MLX5_FLOW_CONTEXT_ACTION_DECAP;
 		mlx5e_tc_set_attr_tx_tun(flow, spec);
 	} else if (tunnel && tunnel->tunnel_type == MLX5E_TC_TUNNEL_TYPE_VXLAN) {
@@ -2258,7 +2272,9 @@ static int parse_tunnel_attr(struct mlx5e_priv *priv,
 
 		tmp_spec = kvzalloc(sizeof(*tmp_spec), GFP_KERNEL);
 		if (!tmp_spec) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack, "Failed to allocate memory");
+#endif
 			netdev_warn(priv->netdev, "Failed to allocate memory");
 			return -ENOMEM;
 		}
@@ -2267,7 +2283,9 @@ static int parse_tunnel_attr(struct mlx5e_priv *priv,
 		err = mlx5e_tc_tun_parse(filter_dev, priv, tmp_spec, f, match_level);
 		if (err) {
 			kvfree(tmp_spec);
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack, "Failed to parse tunnel attributes");
+#endif
 			netdev_warn(priv->netdev, "Failed to parse tunnel attributes");
 			return err;
 		}
@@ -2325,9 +2343,16 @@ static int mlx5e_flower_parse_meta(struct net_device *filter_dev,
 				   struct flow_cls_offload *f)
 {
 	struct flow_rule *rule = flow_cls_offload_flow_rule(f);
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#else
+	struct netlink_ext_ack *extack;
+#endif
 	struct net_device *ingress_dev;
 	struct flow_match_meta match;
+#ifndef HAVE_TC_CLS_OFFLOAD_EXTACK
+	extack = NULL;
+#endif
 
 	if (!flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_META))
 		return 0;
@@ -2363,9 +2388,10 @@ static bool skip_key_basic(struct net_device *filter_dev,
 	 * label fields.  However, the actual ethertype is IP so we want to
 	 * avoid matching on this, otherwise we'll fail the match.
 	 */
+#ifdef HAVE_NET_BAREUDP_H
 	if (netif_is_bareudp(filter_dev) && f->common.chain_index == 0)
 		return true;
-
+#endif
 	return false;
 }
 
@@ -2377,7 +2403,11 @@ static int __parse_cls_flower(struct mlx5e_priv *priv,
 			      u8 *inner_match_level, u8 *outer_match_level,
 			      bool *is_tunnel_flow)
 {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#else
+	struct netlink_ext_ack *extack;
+#endif
 	void *headers_c = MLX5_ADDR_OF(fte_match_param, spec->match_criteria,
 				       outer_headers);
 	void *headers_v = MLX5_ADDR_OF(fte_match_param, spec->match_value,
@@ -2392,6 +2422,9 @@ static int __parse_cls_flower(struct mlx5e_priv *priv,
 	u8 ip_proto = 0;
 	u8 *match_level;
 	int err;
+#ifndef HAVE_TC_CLS_OFFLOAD_EXTACK
+	extack = NULL;
+#endif
 
 	match_level = outer_match_level;
 
@@ -2416,13 +2449,27 @@ static int __parse_cls_flower(struct mlx5e_priv *priv,
 	      BIT(FLOW_DISSECTOR_KEY_ENC_IP) |
 	      BIT(FLOW_DISSECTOR_KEY_ENC_OPTS) |
 	      BIT(FLOW_DISSECTOR_KEY_MPLS))) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack, "Unsupported key");
+#endif
 		netdev_dbg(priv->netdev, "Unsupported key used: 0x%x\n",
 			   dissector->used_keys);
 		return -EOPNOTSUPP;
 	}
 
-	if (mlx5e_get_tc_tun(filter_dev)) {
+#if !defined(HAVE_TC_INDR_API) && !defined(CONFIG_COMPAT_KERNEL_4_14)
+	/* for old kernels we dont have real filter_dev,
+	 * and mlx5e_get_tc_tun always return vxlan
+	 */
+	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ENC_IPV4_ADDRS) ||
+	    flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ENC_IPV6_ADDRS) ||
+	    flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ENC_KEYID) ||
+	    flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ENC_PORTS) ||
+	    flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ENC_OPTS))
+#else
+	if (mlx5e_get_tc_tun(filter_dev))
+#endif
+	{
 		bool match_inner = false;
 
 		err = parse_tunnel_attr(priv, flow, spec, f, filter_dev,
@@ -2712,8 +2759,10 @@ static int __parse_cls_flower(struct mlx5e_priv *priv,
 				 udp_dport, ntohs(match.key->dst));
 			break;
 		default:
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "Only UDP and TCP transports are supported for L4 matching");
+#endif
 			netdev_err(priv->netdev,
 				   "Only UDP and TCP transport are supported\n");
 			return -EINVAL;
@@ -2746,7 +2795,9 @@ static int parse_cls_flower(struct mlx5e_priv *priv,
 			    struct net_device *filter_dev)
 {
 	u8 inner_match_level, outer_match_level, non_tunnel_match_level;
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	struct mlx5_core_dev *dev = priv->mdev;
 	struct mlx5_eswitch *esw = dev->priv.eswitch;
 	struct mlx5e_rep_priv *rpriv = priv->ppriv;
@@ -2771,8 +2822,10 @@ static int parse_cls_flower(struct mlx5e_priv *priv,
 		if (rep->vport != MLX5_VPORT_UPLINK &&
 		    (esw->offloads.inline_mode != MLX5_INLINE_MODE_NONE &&
 		    esw->offloads.inline_mode < non_tunnel_match_level)) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "Flow is not offloaded due to min inline setting");
+#endif
 			netdev_warn(priv->netdev,
 				    "Flow is not offloaded due to min inline setting, required %d actual %d\n",
 				    non_tunnel_match_level, esw->offloads.inline_mode);
@@ -2979,8 +3032,10 @@ static int offload_pedit_fields(struct mlx5e_priv *priv,
 			continue;
 
 		if (s_mask && a_mask) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "can't set and add to the same HW field");
+#endif
 			printk(KERN_WARNING "mlx5: can't set and add to the same HW field (%x)\n", f->field);
 			return -EOPNOTSUPP;
 		}
@@ -3018,8 +3073,10 @@ static int offload_pedit_fields(struct mlx5e_priv *priv,
 		next_z = find_next_zero_bit(&mask, f->field_bsize, first);
 		last  = find_last_bit(&mask, f->field_bsize);
 		if (first < next_z && next_z < last) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "rewrite of few sub-fields isn't supported");
+#endif
 			printk(KERN_WARNING "mlx5: rewrite of few sub-fields (mask %lx) isn't offloaded\n",
 			       mask);
 			return -EOPNOTSUPP;
@@ -3027,8 +3084,10 @@ static int offload_pedit_fields(struct mlx5e_priv *priv,
 
 		err = alloc_mod_hdr_actions(priv->mdev, namespace, mod_acts);
 		if (err) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "too many pedit actions, can't offload");
+#endif
 			mlx5_core_warn(priv->mdev,
 				       "mlx5: parsed %d pedit actions, can't do more\n",
 				       mod_acts->num_actions);
@@ -3218,8 +3277,10 @@ static int alloc_tc_pedit_action(struct mlx5e_priv *priv, int namespace,
 	for (cmd = 0; cmd < __PEDIT_CMD_MAX; cmd++) {
 		cmd_masks = &hdrs[cmd].masks;
 		if (memcmp(cmd_masks, &zero_masks, sizeof(zero_masks))) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "attempt to offload an unsupported field");
+#endif
 			netdev_warn(priv->netdev, "attempt to offload an unsupported field (cmd %d)\n", cmd);
 			print_hex_dump(KERN_WARNING, "mask: ", DUMP_PREFIX_ADDRESS,
 				       16, 1, cmd_masks, sizeof(zero_masks), true);
@@ -3245,16 +3306,20 @@ static bool csum_offload_supported(struct mlx5e_priv *priv,
 
 	/*  The HW recalcs checksums only if re-writing headers */
 	if (!(action & MLX5_FLOW_CONTEXT_ACTION_MOD_HDR)) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "TC csum action is only offloaded with pedit");
+#endif
 		netdev_warn(priv->netdev,
 			    "TC csum action is only offloaded with pedit\n");
 		return false;
 	}
 
 	if (update_flags & ~prot_flags) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "can't offload TC csum action for some header/s");
+#endif
 		netdev_warn(priv->netdev,
 			    "can't offload TC csum action for some header/s - flags %#x\n",
 			    update_flags);
@@ -3384,8 +3449,10 @@ static bool modify_header_match_supported(struct mlx5e_priv *priv,
 	 */
 	if (!ct_clear && modify_tuple &&
 	    mlx5_tc_ct_add_no_trk_match(priv, spec)) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "can't offload tuple modify header with ct matches");
+#endif
 		netdev_info(priv->netdev,
 			    "can't offload tuple modify header with ct matches");
 		return false;
@@ -3394,8 +3461,10 @@ static bool modify_header_match_supported(struct mlx5e_priv *priv,
 	ip_proto = MLX5_GET(fte_match_set_lyr_2_4, headers_v, ip_protocol);
 	if (modify_ip_header && ip_proto != IPPROTO_TCP &&
 	    ip_proto != IPPROTO_UDP && ip_proto != IPPROTO_ICMP) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "can't offload re-write of non TCP/UDP");
+#endif
 		netdev_info(priv->netdev, "can't offload re-write of ip proto %d\n",
 			    ip_proto);
 		return false;
@@ -3535,7 +3604,9 @@ static int parse_tc_nic_actions(struct mlx5e_priv *priv,
 	struct mlx5_flow_attr *attr = flow->attr;
 	struct mlx5_nic_flow_attr *nic_attr;
 	struct pedit_headers_action hdrs[2] = {};
+#ifdef HAVE_FLOW_ACTION_PRIORITY
 	struct mlx5e_tc_table *tc = &priv->fs.tc;
+#endif
 	const struct flow_action_entry *act;
 	bool prio_set = false;
 	u32 action = 0;
@@ -3544,9 +3615,11 @@ static int parse_tc_nic_actions(struct mlx5e_priv *priv,
 	if (!flow_action_has_entries(flow_action))
 		return -EINVAL;
 
+#ifdef HAVE_FLOW_ACTION_HW_STATS_CHECK
 	if (!flow_action_hw_stats_check(flow_action, extack,
 					FLOW_ACTION_HW_STATS_DELAYED_BIT))
 		return -EOPNOTSUPP;
+#endif
 
 	nic_attr = attr->nic_attr;
 
@@ -3599,8 +3672,10 @@ static int parse_tc_nic_actions(struct mlx5e_priv *priv,
 				action |= MLX5_FLOW_CONTEXT_ACTION_FWD_DEST |
 					  MLX5_FLOW_CONTEXT_ACTION_COUNT;
 			} else {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 				NL_SET_ERR_MSG_MOD(extack,
 						   "device is not on same HW, can't offload");
+#endif
 				netdev_warn(priv->netdev, "device %s not on same HW, can't offload\n",
 					    peer_dev->name);
 				return -EINVAL;
@@ -3639,6 +3714,7 @@ static int parse_tc_nic_actions(struct mlx5e_priv *priv,
 			attr->dest_chain = act->chain_index;
 				       }
 			break;
+#ifdef HAVE_FLOW_ACTION_CT
 		case FLOW_ACTION_CT:
 			err = mlx5_tc_ct_parse_action(get_ct_priv(priv), attr, act, extack);
 			if (err)
@@ -3646,6 +3722,8 @@ static int parse_tc_nic_actions(struct mlx5e_priv *priv,
 
 			flow_flag_set(flow, CT);
 			break;
+#endif
+#ifdef HAVE_FLOW_ACTION_PRIORITY
 		case FLOW_ACTION_PRIORITY:
 			if (act->priority > tc->num_prio_hp) {
 				NL_SET_ERR_MSG_MOD(extack, "Skb priority value is out of range");
@@ -3656,6 +3734,7 @@ static int parse_tc_nic_actions(struct mlx5e_priv *priv,
 			prio_set = true;
 			action |= MLX5_FLOW_CONTEXT_ACTION_MOD_HDR;
 			break;
+#endif
 		default:
 			NL_SET_ERR_MSG_MOD(extack, "The offload action is not supported");
 			return -EOPNOTSUPP;
@@ -3830,8 +3909,12 @@ static int add_vlan_pop_action(struct mlx5e_priv *priv,
 	};
 	int nest_level, err = 0;
 
+#ifdef HAVE_NET_DEVICE_HAS_LOWER_LEVEL
 	nest_level = attr->parse_attr->filter_dev->lower_level -
 						priv->netdev->lower_level;
+#else
+	nest_level = vlan_get_encap_level(attr->parse_attr->filter_dev);
+#endif
 	while (nest_level--) {
 		err = parse_tc_vlan_action(priv, &vlan_act, attr->esw_attr, action);
 		if (err)
@@ -3883,8 +3966,10 @@ static bool is_duplicated_output_device(struct net_device *dev,
 
 	for (i = 0; i < if_count; i++) {
 		if (ifindexes[i] == out_dev->ifindex) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "can't duplicate output to same device");
+#endif
 			netdev_err(dev, "can't duplicate output to same device: %s\n",
 				   out_dev->name);
 			return true;
@@ -3963,8 +4048,10 @@ static int verify_uplink_forwarding(struct mlx5e_priv *priv,
 
 	if (!MLX5_CAP_ESW_FLOWTABLE_FDB(esw->dev,
 					termination_table_raw_traffic)) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "devices are both uplink, can't offload forwarding");
+#endif
 			pr_err("devices %s %s are both uplink, can't offload forwarding\n",
 			       priv->netdev->name, out_dev->name);
 			return -EOPNOTSUPP;
@@ -4037,21 +4124,28 @@ static int parse_tc_fdb_actions(struct mlx5e_priv *priv,
 	bool encap = false, decap = false;
 	u32 action = attr->action;
 	int err, i, if_count = 0;
+#if defined(HAVE_IS_TCF_SKBEDIT_PTYPE) || defined(HAVE_IS_TCF_MIRRED_INGRESS_REDIRECT)
 	bool ptype_host = false;
+#endif
+#ifdef HAVE_NET_BAREUDP_H
 	bool mpls_push = false;
+#endif
 
 	if (!flow_action_has_entries(flow_action))
 		return -EINVAL;
 
+#ifdef HAVE_FLOW_ACTION_HW_STATS_CHECK
 	if (!flow_action_hw_stats_check(flow_action, extack,
 					FLOW_ACTION_HW_STATS_DELAYED_BIT))
 		return -EOPNOTSUPP;
+#endif
 
 	parse_attr = attr->parse_attr;
 	esw_attr = attr->esw_attr;
 
 	flow_action_for_each(i, act, flow_action) {
 		switch (act->id) {
+#ifdef HAVE_IS_TCF_SKBEDIT_PTYPE
 		case FLOW_ACTION_PTYPE:
 			if (act->ptype != PACKET_HOST) {
 				NL_SET_ERR_MSG_MOD(extack,
@@ -4062,10 +4156,12 @@ static int parse_tc_fdb_actions(struct mlx5e_priv *priv,
 			ptype_host = true;
 
 			break;
+#endif
 		case FLOW_ACTION_DROP:
 			action |= MLX5_FLOW_CONTEXT_ACTION_DROP |
 				  MLX5_FLOW_CONTEXT_ACTION_COUNT;
 			break;
+#ifdef HAVE_NET_BAREUDP_H
 		case FLOW_ACTION_MPLS_PUSH:
 			if (!MLX5_CAP_ESW_FLOWTABLE_FDB(priv->mdev,
 							reformat_l2_to_l3_tunnel) ||
@@ -4097,6 +4193,7 @@ static int parse_tc_fdb_actions(struct mlx5e_priv *priv,
 			action |= MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT;
 			flow_flag_set(flow, L3_TO_L2_DECAP);
 			break;
+#endif
 		case FLOW_ACTION_MANGLE:
 		case FLOW_ACTION_ADD:
 			err = parse_tc_pedit_action(priv, act, MLX5_FLOW_NAMESPACE_FDB,
@@ -4115,6 +4212,7 @@ static int parse_tc_fdb_actions(struct mlx5e_priv *priv,
 				break;
 
 			return -EOPNOTSUPP;
+#ifdef HAVE_IS_TCF_MIRRED_INGRESS_REDIRECT
 		case FLOW_ACTION_REDIRECT_INGRESS: {
 			struct net_device *out_dev;
 
@@ -4146,6 +4244,7 @@ static int parse_tc_fdb_actions(struct mlx5e_priv *priv,
 
 			break;
 		}
+#endif
 		case FLOW_ACTION_REDIRECT:
 		case FLOW_ACTION_MIRRED: {
 			struct mlx5e_priv *out_priv;
@@ -4160,12 +4259,13 @@ static int parse_tc_fdb_actions(struct mlx5e_priv *priv,
 				return -EINVAL;
 			}
 
+#ifdef HAVE_NET_BAREUDP_H
 			if (mpls_push && !netif_is_bareudp(out_dev)) {
 				NL_SET_ERR_MSG_MOD(extack,
 						   "mpls is supported only through a bareudp device");
 				return -EOPNOTSUPP;
 			}
-
+#endif
 			if (ft_flow && out_dev == priv->netdev) {
 				/* Ignore forward to self rules generated
 				 * by adding both mlx5 devs to the flow table
@@ -4175,8 +4275,10 @@ static int parse_tc_fdb_actions(struct mlx5e_priv *priv,
 			}
 
 			if (esw_attr->out_count >= MLX5_MAX_FLOW_FWD_VPORTS) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 				NL_SET_ERR_MSG_MOD(extack,
 						   "can't support more output ports, can't offload forwarding");
+#endif
 				netdev_warn(priv->netdev,
 					    "can't support more than %d output ports, can't offload forwarding\n",
 					    esw_attr->out_count);
@@ -4263,8 +4365,10 @@ static int parse_tc_fdb_actions(struct mlx5e_priv *priv,
 				 */
 				return -EINVAL;
 			} else {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 				NL_SET_ERR_MSG_MOD(extack,
 						   "devices are not on same switch HW, can't offload forwarding");
+#endif
 				pr_err_once("devices %s %s not on same switch HW, can't offload forwarding\n",
 					    priv->netdev->name, out_dev->name);
 				pr_debug("devices %s %s not on same switch HW, can't offload forwarding\n",
@@ -4321,6 +4425,7 @@ static int parse_tc_fdb_actions(struct mlx5e_priv *priv,
 			action |= MLX5_FLOW_CONTEXT_ACTION_COUNT;
 			attr->dest_chain = act->chain_index;
 			break;
+#ifdef HAVE_FLOW_ACTION_CT
 		case FLOW_ACTION_CT:
 			if (flow_flag_test(flow, SAMPLE)) {
 				NL_SET_ERR_MSG_MOD(extack, "Sample action with connection tracking is not supported");
@@ -4333,6 +4438,7 @@ static int parse_tc_fdb_actions(struct mlx5e_priv *priv,
 			flow_flag_set(flow, CT);
 			esw_attr->split_count = esw_attr->out_count;
 			break;
+#endif
 #if IS_ENABLED(CONFIG_MLX5_TC_SAMPLE)
 		case FLOW_ACTION_SAMPLE:
 			if (flow_flag_test(flow, CT)) {
@@ -4409,8 +4515,10 @@ static int parse_tc_fdb_actions(struct mlx5e_priv *priv,
 			 * device.
 			 */
 
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 			NL_SET_ERR_MSG(extack,
 				       "Decap with goto isn't supported");
+#endif
 			netdev_warn(priv->netdev,
 				    "Decap with goto isn't supported");
 			return -EOPNOTSUPP;
@@ -4427,8 +4535,10 @@ static int parse_tc_fdb_actions(struct mlx5e_priv *priv,
 	}
 
 	if (esw_attr->split_count > 0 && !mlx5_esw_has_fwd_fdb(priv->mdev)) {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "current firmware doesn't support split rule for port mirroring");
+#endif
 		netdev_warn_once(priv->netdev, "current firmware doesn't support split rule for port mirroring\n");
 		return -EOPNOTSUPP;
 	}
@@ -4474,14 +4584,46 @@ static const struct rhashtable_params tc_ht_params = {
 	.automatic_shrinking = true,
 };
 
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+static void get_new_flags(struct mlx5e_priv *priv, unsigned long *flags)
+{
+	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+
+	if (mlx5e_eswitch_rep(priv->netdev) &&
+	    MLX5_VPORT_MANAGER(priv->mdev) && esw->mode == MLX5_ESWITCH_OFFLOADS)
+		*flags |= MLX5_TC_FLAG(ESW_OFFLOAD);
+}
+#elif !defined(HAVE_TC_BLOCK_OFFLOAD) && !defined(HAVE_FLOW_BLOCK_OFFLOAD)
+static void get_new_flags(struct mlx5e_priv *priv, unsigned long *flags)
+{
+	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+
+	if (esw && esw->mode == MLX5_ESWITCH_OFFLOADS)
+		*flags |= MLX5_TC_FLAG(ESW_OFFLOAD);
+}
+#endif
+
 static struct rhashtable *get_tc_ht(struct mlx5e_priv *priv,
 				    unsigned long flags)
 {
 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 	struct mlx5e_rep_priv *uplink_rpriv;
 
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+	if (mlx5e_eswitch_rep(priv->netdev) &&
+	    MLX5_VPORT_MANAGER(priv->mdev) && esw->mode == MLX5_ESWITCH_OFFLOADS) {
+#elif !defined(HAVE_TC_BLOCK_OFFLOAD) && !defined(HAVE_FLOW_BLOCK_OFFLOAD)
+	if ((flags & MLX5_TC_FLAG(ESW_OFFLOAD)) ||
+	    (esw && esw->mode == MLX5_ESWITCH_OFFLOADS)) {
+#else
 	if (flags & MLX5_TC_FLAG(ESW_OFFLOAD)) {
+#endif
 		uplink_rpriv = mlx5_eswitch_get_uplink_priv(esw, REP_ETH);
+#if !defined(CONFIG_COMPAT_CLS_FLOWER_MOD) && \
+    !defined(HAVE_TC_BLOCK_OFFLOAD) && !defined(HAVE_FLOW_BLOCK_OFFLOAD)
+		if (!uplink_rpriv->uplink_priv.tc_ht.tbl)
+			return &priv->fs.tc.ht;
+#endif
 		return &uplink_rpriv->uplink_priv.tc_ht;
 	} else /* NIC offload */
 		return &priv->fs.tc.ht;
@@ -4490,22 +4632,32 @@ static struct rhashtable *get_tc_ht(struct mlx5e_priv *priv,
 static bool is_peer_flow_needed(struct mlx5e_tc_flow *flow)
 {
 	struct mlx5_esw_flow_attr *esw_attr = flow->attr->esw_attr;
+#ifdef HAVE_QDISC_SUPPORTS_BLOCK_SHARING
 	bool is_rep_ingress = esw_attr->in_rep->vport != MLX5_VPORT_UPLINK &&
 		flow_flag_test(flow, INGRESS);
 	bool act_is_encap = !!(flow->attr->action &
 			       MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT);
+#endif
 	bool esw_paired = mlx5_devcom_is_paired(esw_attr->in_mdev->priv.devcom,
 						MLX5_DEVCOM_ESW_OFFLOADS);
 
 	if (!esw_paired)
 		return false;
 
+#ifdef HAVE_QDISC_SUPPORTS_BLOCK_SHARING
 	if ((mlx5_lag_is_sriov(esw_attr->in_mdev) ||
 	     mlx5_lag_is_multipath(esw_attr->in_mdev)) &&
-	    (is_rep_ingress || act_is_encap))
+	    (is_rep_ingress || act_is_encap
+#ifdef HAVE_TC_SETUP_CB_EGDEV_REGISTER
+	     || (flow->flags & MLX5_TC_FLAG(EGRESS))
+#endif
+	    ))
 		return true;
 
 	return false;
+#else
+	return (mlx5_lag_is_sriov(esw_attr->in_mdev) ||  mlx5_lag_is_multipath(esw_attr->in_mdev));
+#endif
 }
 
 struct mlx5_flow_attr *
@@ -4571,8 +4723,16 @@ mlx5e_flow_attr_init(struct mlx5_flow_attr *attr,
 		     struct flow_cls_offload *f)
 {
 	attr->parse_attr = parse_attr;
+#ifdef CONFIG_COMPAT_PRIO_CHAIN_SUPPORT
 	attr->chain = f->common.chain_index;
+#ifdef CONFIG_COMPAT_TC_PRIO_IS_MAJOR
 	attr->prio = f->common.prio;
+#else
+	attr->prio = TC_H_MAJ(f->common.prio) >> 16;
+#endif
+#else
+	attr->prio = 1;
+#endif
 }
 
 static void
@@ -4606,12 +4766,24 @@ __mlx5e_add_fdb_flow(struct mlx5e_priv *priv,
 		     struct mlx5_eswitch_rep *in_rep,
 		     struct mlx5_core_dev *in_mdev)
 {
-	struct flow_rule *rule = flow_cls_offload_flow_rule(f);
+	struct flow_rule *rule;
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#else
+	struct netlink_ext_ack *extack = NULL;
+#endif
 	struct mlx5e_tc_flow_parse_attr *parse_attr;
 	struct mlx5e_tc_flow *flow;
 	int attr_size, err;
 
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+	rule = alloc_flow_rule(&f);
+	if (IS_ERR(rule))
+		return ERR_PTR(PTR_ERR(rule));
+#else
+	rule = flow_cls_offload_flow_rule(f);
+#endif
+
 	flow_flags |= BIT(MLX5E_TC_FLOW_FLAG_ESWITCH);
 	attr_size  = sizeof(struct mlx5_esw_flow_attr);
 	err = mlx5e_alloc_flow(priv, attr_size, f, flow_flags,
@@ -4653,12 +4825,19 @@ __mlx5e_add_fdb_flow(struct mlx5e_priv *priv,
 		add_unready_flow(flow);
 	}
 
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+	free_flow_rule(rule);
+#endif
+
 	return flow;
 
 err_free:
 	dealloc_mod_hdr_actions(&parse_attr->mod_hdr_acts);
 	mlx5e_flow_put(priv, flow);
 out:
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+        free_flow_rule(rule);
+#endif
 	return ERR_PTR(err);
 }
 
@@ -4754,18 +4933,32 @@ mlx5e_add_nic_flow(struct mlx5e_priv *priv,
 		   struct net_device *filter_dev,
 		   struct mlx5e_tc_flow **__flow)
 {
-	struct flow_rule *rule = flow_cls_offload_flow_rule(f);
+	struct flow_rule *rule;
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#else
+	struct netlink_ext_ack *extack = NULL;
+#endif
 	struct mlx5e_tc_flow_parse_attr *parse_attr;
 	struct mlx5e_tc_flow *flow;
 	int attr_size, err;
 
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+	rule = alloc_flow_rule(&f);
+	if (IS_ERR(rule))
+		return PTR_ERR(rule);
+#else
+	rule = flow_cls_offload_flow_rule(f);
+#endif
+
+#if defined(HAVE_TC_CLS_OFFLOAD_EXTACK) && defined(CONFIG_COMPAT_PRIO_CHAIN_SUPPORT)
 	if (!MLX5_CAP_FLOWTABLE_NIC_RX(priv->mdev, ignore_flow_level)) {
 		if (!tc_cls_can_offload_and_chain0(priv->netdev, &f->common))
 			return -EOPNOTSUPP;
 	} else if (!tc_can_offload_extack(priv->netdev, f->common.extack)) {
 		return -EOPNOTSUPP;
 	}
+#endif
 
 	flow_flags |= BIT(MLX5E_TC_FLOW_FLAG_NIC);
 	attr_size  = sizeof(struct mlx5_nic_flow_attr);
@@ -4798,6 +4991,9 @@ mlx5e_add_nic_flow(struct mlx5e_priv *priv,
 		goto err_free;
 
 	flow_flag_set(flow, OFFLOADED);
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+        free_flow_rule(rule);
+#endif
 	*__flow = flow;
 
 	return 0;
@@ -4806,6 +5002,9 @@ err_free:
 	dealloc_mod_hdr_actions(&parse_attr->mod_hdr_acts);
 	mlx5e_flow_put(priv, flow);
 out:
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+        free_flow_rule(rule);
+#endif
 	return err;
 }
 
@@ -4822,8 +5021,10 @@ mlx5e_tc_add_flow(struct mlx5e_priv *priv,
 
 	get_flags(flags, &flow_flags);
 
+#if defined(HAVE_TC_CLS_OFFLOAD_EXTACK) && defined(HAVE_TC_CLS_FLOWER_OFFLOAD_COMMON)
 	if (!tc_can_offload_extack(priv->netdev, f->common.extack))
 		return -EOPNOTSUPP;
+#endif
 
 	if (esw && esw->mode == MLX5_ESWITCH_OFFLOADS)
 		err = mlx5e_add_fdb_flow(priv, f, flow_flags,
@@ -4863,12 +5064,19 @@ int mlx5e_configure_flower(struct net_device *dev, struct mlx5e_priv *priv,
 			   struct flow_cls_offload *f, unsigned long flags)
 {
 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	struct rhashtable *tc_ht = get_tc_ht(priv, flags);
 	struct mlx5e_rep_priv *rpriv = priv->ppriv;
 	struct mlx5e_tc_flow *flow;
 	int err = 0;
 
+#if defined(CONFIG_COMPAT_CLS_FLOWER_MOD) || \
+    (!defined(HAVE_TC_BLOCK_OFFLOAD) && !defined(HAVE_FLOW_BLOCK_OFFLOAD))
+	get_new_flags(priv, &flags);
+#endif
+
 	if (get_esw_tc_refcnt(esw))
 		return -EOPNOTSUPP;
 
@@ -4881,8 +5089,15 @@ int mlx5e_configure_flower(struct net_device *dev, struct mlx5e_priv *priv,
 		if (is_flow_rule_duplicate_allowed(dev, rpriv) && flow->orig_dev != dev)
 			goto rcu_unlock;
 
+#if !defined(HAVE_TC_INDR_API)
+		if(flow->orig_dev != dev)
+			goto out;
+#endif
+
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "flow cookie already exists, ignoring");
+#endif
 		netdev_warn_once(priv->netdev,
 				 "flow cookie %lx already exists, ignoring\n",
 				 f->cookie);
@@ -4894,7 +5109,9 @@ rcu_unlock:
 	if (flow)
 		goto out;
 
+#ifndef MLX_DISABLE_TRACEPOINTS
 	trace_mlx5e_configure_flower(f);
+#endif
 	err = mlx5e_tc_add_flow(priv, f, flags, dev, &flow);
 	if (err)
 		goto out;
@@ -4917,6 +5134,9 @@ out:
 	put_esw_tc_refcnt(esw);
 	return err;
 }
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+EXPORT_SYMBOL(mlx5e_configure_flower);
+#endif
 
 static bool same_flow_direction(struct mlx5e_tc_flow *flow, int flags)
 {
@@ -4952,7 +5172,9 @@ int mlx5e_delete_flower(struct net_device *dev, struct mlx5e_priv *priv,
 	rhashtable_remove_fast(tc_ht, &flow->node, tc_ht_params);
 	rcu_read_unlock();
 
+#ifndef MLX_DISABLE_TRACEPOINTS
 	trace_mlx5e_delete_flower(f);
+#endif
 	mlx5e_flow_put(priv, flow);
 
 	put_esw_tc_refcnt(esw);
@@ -4962,6 +5184,9 @@ errout:
 	rcu_read_unlock();
 	return err;
 }
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+EXPORT_SYMBOL(mlx5e_delete_flower);
+#endif
 
 int mlx5e_stats_flower(struct net_device *dev, struct mlx5e_priv *priv,
 		       struct flow_cls_offload *f, unsigned long flags)
@@ -4971,11 +5196,21 @@ int mlx5e_stats_flower(struct net_device *dev, struct mlx5e_priv *priv,
 	struct mlx5_eswitch *peer_esw;
 	struct mlx5e_tc_flow *flow;
 	struct mlx5_fc *counter;
+#if !defined(HAVE_TC_CLS_FLOWER_OFFLOAD_HAS_STATS_FIELD) && \
+    !defined(HAVE_TCF_EXTS_STATS_UPDATE)
+	struct tc_action *a;
+	LIST_HEAD(actions);
+#endif
 	u64 lastuse = 0;
 	u64 packets = 0;
 	u64 bytes = 0;
 	int err = 0;
 
+#if defined(CONFIG_COMPAT_CLS_FLOWER_MOD) || \
+    (!defined(HAVE_TC_BLOCK_OFFLOAD) && !defined(HAVE_FLOW_BLOCK_OFFLOAD))
+	get_new_flags(priv, &flags);
+#endif
+
 	rcu_read_lock();
 	flow = mlx5e_flow_get(rhashtable_lookup(tc_ht, &f->cookie,
 						tc_ht_params));
@@ -5022,14 +5257,52 @@ int mlx5e_stats_flower(struct net_device *dev, struct mlx5e_priv *priv,
 no_peer_counter:
 	mlx5_devcom_release_peer_data(devcom, MLX5_DEVCOM_ESW_OFFLOADS);
 out:
+#ifdef HAVE_FLOW_STATS_UPDATE_6_PARAMS
 	flow_stats_update(&f->stats, bytes, packets, 0, lastuse,
 			  FLOW_ACTION_HW_STATS_DELAYED);
+#elif defined(HAVE_FLOW_STATS_UPDATE_5_PARAMS)
+	flow_stats_update(&f->stats, bytes, packets, lastuse,
+			  FLOW_ACTION_HW_STATS_DELAYED);
+#elif defined(HAVE_TC_CLS_FLOWER_OFFLOAD_HAS_STATS_FIELD)
+	flow_stats_update(&f->stats, bytes, packets, lastuse);
+#elif defined(HAVE_TCF_EXTS_STATS_UPDATE)
+	tcf_exts_stats_update(f->exts, bytes, packets, lastuse);
+#else
+	preempt_disable();
+
+#ifdef HAVE_TCF_EXTS_TO_LIST
+	tcf_exts_to_list(f->exts, &actions);
+	list_for_each_entry(a, &actions, list)
+#else
+	tc_for_each_action(a, f->exts)
+#endif
+#ifdef HAVE_TCF_ACTION_STATS_UPDATE
+		tcf_action_stats_update(a, bytes, packets, lastuse);
+#else
+	{
+		struct tcf_act_hdr *h = a->priv;
+
+		spin_lock(&h->tcf_lock);
+		h->tcf_tm.lastuse = max_t(u64, h->tcf_tm.lastuse, lastuse);
+		h->tcf_bstats.bytes += bytes;
+		h->tcf_bstats.packets += packets;
+		spin_unlock(&h->tcf_lock);
+	}
+#endif
+	preempt_enable();
+#endif /* HAVE_TC_CLS_FLOWER_OFFLOAD_HAS_STATS_FIELD */
+#ifndef MLX_DISABLE_TRACEPOINTS
 	trace_mlx5e_stats_flower(f);
+#endif
 errout:
 	mlx5e_flow_put(priv, flow);
 	return err;
 }
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+EXPORT_SYMBOL(mlx5e_stats_flower);
+#endif
 
+#ifdef HAVE_TC_CLSMATCHALL_STATS
 static int apply_police_params(struct mlx5e_priv *priv, u64 rate,
 			       struct netlink_ext_ack *extack)
 {
@@ -5085,8 +5358,10 @@ static int scan_tc_matchall_fdb_actions(struct mlx5e_priv *priv,
 		return -EOPNOTSUPP;
 	}
 
+#ifdef HAVE_FLOW_ACTION_HW_STATS_CHECK
 	if (!flow_action_basic_hw_stats_check(flow_action, extack))
 		return -EOPNOTSUPP;
+#endif
 
 	flow_action_for_each(i, act, flow_action) {
 		switch (act->id) {
@@ -5111,18 +5386,38 @@ int mlx5e_tc_configure_matchall(struct mlx5e_priv *priv,
 {
 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 	struct netlink_ext_ack *extack = ma->common.extack;
+	int prio = ma->common.prio;
+	struct flow_rule *rule;
+	int err;
+
+#ifndef CONFIG_COMPAT_TC_PRIO_IS_MAJOR
+	prio = TC_H_MAJ(prio) >> 16;
+#endif
 
 	if (!mlx5_esw_qos_enabled(esw)) {
 		NL_SET_ERR_MSG_MOD(extack, "QoS is not supported on this device");
 		return -EOPNOTSUPP;
 	}
 
-	if (ma->common.prio != 1) {
+	if (prio != 1) {
 		NL_SET_ERR_MSG_MOD(extack, "only priority 1 is supported");
 		return -EINVAL;
 	}
 
-	return scan_tc_matchall_fdb_actions(priv, &ma->rule->action, extack);
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+	rule = __alloc_flow_rule(ma->exts, NULL, 0);
+	if (IS_ERR(rule))
+		return PTR_ERR(rule);
+#else
+	rule = ma->rule;
+#endif
+
+	err = scan_tc_matchall_fdb_actions(priv, &rule->action, extack);
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+	free_flow_rule(rule);
+#endif
+
+	return err;
 }
 
 int mlx5e_tc_delete_matchall(struct mlx5e_priv *priv,
@@ -5145,9 +5440,19 @@ void mlx5e_tc_stats_matchall(struct mlx5e_priv *priv,
 	dpkts = cur_stats.rx_packets - rpriv->prev_vf_vport_stats.rx_packets;
 	dbytes = cur_stats.rx_bytes - rpriv->prev_vf_vport_stats.rx_bytes;
 	rpriv->prev_vf_vport_stats = cur_stats;
+#ifdef HAVE_FLOW_STATS_UPDATE_6_PARAMS
 	flow_stats_update(&ma->stats, dbytes, dpkts, 0, jiffies,
 			  FLOW_ACTION_HW_STATS_DELAYED);
+#elif defined(HAVE_FLOW_STATS_UPDATE_5_PARAMS)
+	flow_stats_update(&ma->stats, dbytes, dpkts, jiffies,
+			  FLOW_ACTION_HW_STATS_DELAYED);
+#elif defined(HAVE_TC_SETUP_FLOW_ACTION)
+	flow_stats_update(&ma->stats, dbytes, dpkts, jiffies);
+#else
+	tcf_exts_stats_update(ma->exts, dbytes, dpkts, jiffies);
+#endif
 }
+#endif /* HAVE_TC_CLSMATCHALL_STATS */
 
 static void mlx5e_tc_hairpin_update_dead_peer(struct mlx5e_priv *priv,
 					      struct mlx5e_priv *peer_priv)
@@ -5423,6 +5728,11 @@ void mlx5e_tc_esw_cleanup(struct rhashtable *tc_ht)
 
 	rhashtable_free_and_destroy(tc_ht, _mlx5e_tc_del_flow, NULL);
 
+#if !defined (CONFIG_COMPAT_CLS_FLOWER_MOD) && \
+    !defined(HAVE_TC_BLOCK_OFFLOAD) && !defined(HAVE_FLOW_BLOCK_OFFLOAD)
+	tc_ht->tbl = NULL;
+#endif
+
 	mapping_destroy(uplink_priv->tunnel_enc_opts_mapping);
 	mapping_destroy(uplink_priv->tunnel_mapping);
 
@@ -5461,10 +5771,28 @@ void mlx5e_tc_reoffload_flows_work(struct work_struct *work)
 	mutex_unlock(&rpriv->unready_flows_lock);
 }
 
+#if defined(HAVE_NDO_SETUP_TC_TAKES_TC_SETUP_TYPE) || defined(HAVE_NDO_SETUP_TC_RH_EXTENDED)
+#ifdef CONFIG_MLX5_ESWITCH
+#if defined(HAVE_TC_BLOCK_OFFLOAD) || defined(HAVE_FLOW_BLOCK_OFFLOAD)
 static int mlx5e_setup_tc_cls_flower(struct mlx5e_priv *priv,
+#else
+int mlx5e_setup_tc_cls_flower(struct net_device *dev,
+#endif
 				     struct flow_cls_offload *cls_flower,
 				     unsigned long flags)
 {
+#ifndef HAVE_TC_CLS_CAN_OFFLOAD_AND_CHAIN0
+#ifdef HAVE_TC_BLOCK_OFFLOAD
+	if (cls_flower->common.chain_index)
+#else
+	struct mlx5e_priv *priv = netdev_priv(dev);
+
+	if (!is_classid_clsact_ingress(cls_flower->common.classid) ||
+	    cls_flower->common.chain_index)
+#endif
+		return -EOPNOTSUPP;
+#endif
+
 	switch (cls_flower->command) {
 	case FLOW_CLS_REPLACE:
 		return mlx5e_configure_flower(priv->netdev, priv, cls_flower,
@@ -5480,6 +5808,7 @@ static int mlx5e_setup_tc_cls_flower(struct mlx5e_priv *priv,
 	}
 }
 
+#if defined(HAVE_TC_BLOCK_OFFLOAD) || defined(HAVE_FLOW_BLOCK_OFFLOAD)
 int mlx5e_setup_tc_block_cb(enum tc_setup_type type, void *type_data,
 			    void *cb_priv)
 {
@@ -5489,6 +5818,11 @@ int mlx5e_setup_tc_block_cb(enum tc_setup_type type, void *type_data,
 	if (!priv->netdev || !netif_device_present(priv->netdev))
 		return -EOPNOTSUPP;
 
+#if defined(HAVE_TC_CLS_OFFLOAD_EXTACK) && !defined(CONFIG_COMPAT_PRIO_CHAIN_SUPPORT)
+	if (!tc_cls_can_offload_and_chain0(priv->netdev, type_data))
+		return -EOPNOTSUPP;
+#endif
+
 	if (mlx5e_is_uplink_rep(priv))
 		flags |= MLX5_TC_FLAG(ESW_OFFLOAD);
 	else
@@ -5568,3 +5902,33 @@ bool mlx5e_tc_update_skb(struct mlx5_cqe64 *cqe,
 
 	return true;
 }
+
+#ifndef HAVE_FLOW_BLOCK_CB_SETUP_SIMPLE
+int mlx5e_setup_tc_block(struct net_device *dev,
+			 struct tc_block_offload *f)
+{
+	struct mlx5e_priv *priv = netdev_priv(dev);
+
+	if (f->binder_type != FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS)
+		return -EOPNOTSUPP;
+
+	switch (f->command) {
+	case TC_BLOCK_BIND:
+		return tcf_block_cb_register(f->block, mlx5e_setup_tc_block_cb,
+					     priv, priv
+#ifdef HAVE_TC_BLOCK_OFFLOAD_EXTACK
+					     , f->extack
+#endif
+					    );
+	case TC_BLOCK_UNBIND:
+		tcf_block_cb_unregister(f->block, mlx5e_setup_tc_block_cb,
+					priv);
+		return 0;
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+#endif /* HAVE_FLOW_BLOCK_CB_SETUP_SIMPLE */
+#endif /* HAVE_TC_BLOCK_OFFLOAD || HAVE_FLOW_BLOCK_OFFLOAD */
+#endif /*ESWITCH */
+#endif
