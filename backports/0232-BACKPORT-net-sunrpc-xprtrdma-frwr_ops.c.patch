From: Tom Wu <tomwu@nvidia.com>
Subject: [PATCH] BACKPORT: net/sunrpc/xprtrdma/frwr_ops.c

Change-Id: I310d4d9a49420e68fabd73295fb5d65508db3b42
---
 net/sunrpc/xprtrdma/frwr_ops.c | 35 +++++++++++++++++++++++++++++++++-
 1 file changed, 34 insertions(+), 1 deletion(-)

diff --git a/net/sunrpc/xprtrdma/frwr_ops.c b/net/sunrpc/xprtrdma/frwr_ops.c
index xxxxxxx..xxxxxxx 100644
--- a/net/sunrpc/xprtrdma/frwr_ops.c
+++ b/net/sunrpc/xprtrdma/frwr_ops.c
@@ -43,7 +43,9 @@
 #include <linux/sunrpc/svc_rdma.h>
 
 #include "xprt_rdma.h"
+#ifdef HAVE_TRACE_RPCRDMA_H
 #include <trace/events/rpcrdma.h>
+#endif
 
 #ifdef CONFIG_NVFS
 #define NVFS_FRWR
@@ -65,8 +67,10 @@ void frwr_release_mr(struct rpcrdma_mr *mr)
 	int rc;
 
 	rc = ib_dereg_mr(mr->frwr.fr_mr);
+#ifdef HAVE_TRACE_RPCRDMA_H
 	if (rc)
 		trace_xprtrdma_frwr_dereg(mr, rc);
+#endif
 	kfree(mr->mr_sg);
 	kfree(mr);
 }
@@ -75,10 +79,14 @@ static void frwr_mr_recycle(struct rpcrdma_mr *mr)
 {
 	struct rpcrdma_xprt *r_xprt = mr->mr_xprt;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_mr_recycle(mr);
+#endif
 
 	if (mr->mr_dir != DMA_NONE) {
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_mr_unmap(mr);
+#endif
 #ifdef CONFIG_NVFS
 		if (rpcrdma_nvfs_unmap_data(r_xprt->rx_ep->re_id->device->dma_device,
 					    mr->mr_sg, mr->mr_nents, mr->mr_dir))
@@ -153,7 +161,9 @@ int frwr_mr_init(struct rpcrdma_xprt *r_xprt, struct rpcrdma_mr *mr)
 
 out_mr_err:
 	rc = PTR_ERR(frmr);
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_frwr_alloc(mr, rc);
+#endif
 	return rc;
 
 out_list_err:
@@ -365,17 +375,23 @@ struct rpcrdma_mr_seg *frwr_map(struct rpcrdma_xprt *r_xprt,
 	mr->mr_handle = ibmr->rkey;
 	mr->mr_length = ibmr->length;
 	mr->mr_offset = ibmr->iova;
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_mr_map(mr);
+#endif
 
 	return seg;
 
 out_dmamap_err:
 	mr->mr_dir = DMA_NONE;
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_frwr_sgerr(mr, i);
+#endif
 	return ERR_PTR(-EIO);
 
 out_mapmr_err:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_frwr_maperr(mr, n);
+#endif
 	return ERR_PTR(-EIO);
 }
 
@@ -387,6 +403,7 @@ out_mapmr_err:
  */
 static void frwr_wc_fastreg(struct ib_cq *cq, struct ib_wc *wc)
 {
+#ifdef HAVE_TRACE_RPCRDMA_H
 	struct ib_cqe *cqe = wc->wr_cqe;
 	struct rpcrdma_frwr *frwr =
 		container_of(cqe, struct rpcrdma_frwr, fr_cqe);
@@ -394,7 +411,7 @@ static void frwr_wc_fastreg(struct ib_cq *cq, struct ib_wc *wc)
 	/* WARNING: Only wr_cqe and status are reliable at this point */
 	trace_xprtrdma_wc_fastreg(wc, frwr);
 	/* The MR will get recycled when the associated req is retransmitted */
-
+#endif
 	rpcrdma_flush_disconnect(cq->cq_context, wc);
 }
 
@@ -449,7 +466,9 @@ void frwr_reminv(struct rpcrdma_rep *rep, struct list_head *mrs)
 	list_for_each_entry(mr, mrs, mr_list)
 		if (mr->mr_handle == rep->rr_inv_rkey) {
 			list_del_init(&mr->mr_list);
+#ifdef HAVE_TRACE_RPCRDMA_H
 			trace_xprtrdma_mr_reminv(mr);
+#endif
 			rpcrdma_mr_put(mr);
 			break;	/* only one invalidated MR per RPC */
 		}
@@ -477,7 +496,9 @@ static void frwr_wc_localinv(struct ib_cq *cq, struct ib_wc *wc)
 	struct rpcrdma_mr *mr = container_of(frwr, struct rpcrdma_mr, frwr);
 
 	/* WARNING: Only wr_cqe and status are reliable at this point */
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_wc_li(wc, frwr);
+#endif
 	__frwr_release_mr(wc, mr);
 
 	rpcrdma_flush_disconnect(cq->cq_context, wc);
@@ -498,7 +519,9 @@ static void frwr_wc_localinv_wake(struct ib_cq *cq, struct ib_wc *wc)
 	struct rpcrdma_mr *mr = container_of(frwr, struct rpcrdma_mr, frwr);
 
 	/* WARNING: Only wr_cqe and status are reliable at this point */
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_wc_li_wake(wc, frwr);
+#endif
 	__frwr_release_mr(wc, mr);
 	complete(&frwr->fr_linv_done);
 
@@ -533,7 +556,9 @@ void frwr_unmap_sync(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req)
 	prev = &first;
 	while ((mr = rpcrdma_mr_pop(&req->rl_registered))) {
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_mr_localinv(mr);
+#endif
 		r_xprt->rx_stats.local_inv_needed++;
 
 		frwr = &mr->frwr;
@@ -576,7 +601,9 @@ void frwr_unmap_sync(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req)
 
 	/* Recycle MRs in the LOCAL_INV chain that did not get posted.
 	 */
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_post_linv(req, rc);
+#endif
 	while (bad_wr) {
 		frwr = container_of(bad_wr, struct rpcrdma_frwr,
 				    fr_invwr);
@@ -603,7 +630,9 @@ static void frwr_wc_localinv_done(struct ib_cq *cq, struct ib_wc *wc)
 	struct rpcrdma_rep *rep = mr->mr_req->rl_reply;
 
 	/* WARNING: Only wr_cqe and status are reliable at this point */
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_wc_li_done(wc, frwr);
+#endif
 	__frwr_release_mr(wc, mr);
 
 	/* Ensure @rep is generated before __frwr_release_mr */
@@ -638,7 +667,9 @@ void frwr_unmap_async(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req)
 	prev = &first;
 	while ((mr = rpcrdma_mr_pop(&req->rl_registered))) {
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_mr_localinv(mr);
+#endif
 		r_xprt->rx_stats.local_inv_needed++;
 
 		frwr = &mr->frwr;
@@ -674,7 +705,9 @@ void frwr_unmap_async(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req)
 
 	/* Recycle MRs in the LOCAL_INV chain that did not get posted.
 	 */
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_post_linv(req, rc);
+#endif
 	while (bad_wr) {
 		frwr = container_of(bad_wr, struct rpcrdma_frwr, fr_invwr);
 		mr = container_of(frwr, struct rpcrdma_mr, frwr);
